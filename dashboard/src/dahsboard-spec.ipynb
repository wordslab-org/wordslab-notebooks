{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application description\n",
    "\n",
    "wordslab-notebooks is an open source project which enables tech enthusiasts to explore, learn, and build AI solutions on their own machine(s).\n",
    "\n",
    "It is a set of shell scripts which automates the creation of a Linux virtual machine and then installs a consistent and pre-configured development environment with Jupyterlab, VS code server, and Open WebUI.\n",
    "\n",
    "The goal of this project is to develop a dashboard to manage the resources of this Linux virtual machine and the configuration of the development environment.\n",
    "\n",
    "The dashboard will be developed in Python with the FastHTML and monsterui librairies. Its code should stay very simple, with as few lines of code as possible.\n",
    "\n",
    "The dashboard is vertically divided into three parts which are decomposed as follows:\n",
    "- Machines resources: 1. Hosting platform, OS, wordslab and apps versions, link to the documentation | 2. CPU name and cores, CPU usage %, RAM capacity and usage % | 3. GPU name and Flops, GPU usage %, VRAM capacity and usage % | 4. Disks capacity and usage % | 5. Network interface IP address and bandwidth usage % (input/output)\n",
    "- Development environment config: 1. List of links to the apps launched inside the virtual machine | 2. List of projects (= git repositories in the workspace directory), with their disk size and git changes and remote synch status | 3. List of models downloaded from Ollama / Huggingface / fastai / Pytorch / Keras, and the size of each model on disk\n",
    "- AI services config: 1. List and status of local AI model endpoints, with their name, description, and status | 2. List of external APIs to AI models, with their URL, API Key, and status | 3. List of external virtual machines with their characteristics, create/start/stop/buttons, status, Url to dashboard, price and cost in $\n",
    "\n",
    "All of these views are automatically updated every 5 seconds based on the virtual machine status.\n",
    "\n",
    "Some of these views feature a create/download form at the top and a delete button for each list element:\n",
    "- 2.2. List of project: form to clone a repo from Github\n",
    "- 2.3. List of models: form to download a new model from the selected provider\n",
    "- 3.1. List of local AI model endpoints: form to load a model in memory or start a model server\n",
    "- 3.2. List of external APIs: form to configure a new extrenal API\n",
    "- 3.3. List of external virtual machines: form to configure a hosting platform and VM template\n",
    "\n",
    "The configuration data is stored in a sqlite database which is accessed from Python with the fastlite library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Resources Data Model\n",
    "\n",
    "### 1. System Information\n",
    "- hosting_platform: Platform identification (str) [update: on startup]\n",
    "- os_version: Operating system details (str) [update: on startup]\n",
    "- wordslab_version: Software version (str) [update: on startup]\n",
    "- apps_versions: Dictionary of installed apps and versions (json) [update: on startup]\n",
    "- docs_url: Documentation link (str) [update: on startup]\n",
    "\n",
    "### 2. CPU Metrics\n",
    "- cpu_model: Processor name/model (str) [update: on startup]\n",
    "- cpu_cores: Number of cores (int) [update: on startup]\n",
    "- cpu_usage: Current CPU utilization percentage (float) [update: every 5s]\n",
    "- ram_total: Total RAM in GB (float) [update: on startup]\n",
    "- ram_used: Current RAM usage percentage (float) [update: every 5s\n",
    "\n",
    "### 3. GPU Metrics\n",
    "- gpu_model: GPU name/model (str) [update: on startup]\n",
    "- gpu_flops: Theoretical FLOPS capacity (float) [update: on startup]\n",
    "- gpu_usage: Current GPU utilization percentage (float) [update: every 5s]\n",
    "- vram_total: Total VRAM in GB (float) [update: on startup]\n",
    "- vram_used: Current VRAM usage percentage (float) [update: every 5s]\n",
    "\n",
    "### 4. Storage Metrics\n",
    "[Repeated for disk_id in (1,2,3,4)]\n",
    "- disk_{id}_name: Disk name/mount point (str) [update: on startup]\n",
    "- disk_{id}_total: Total disk space in GB (float) [update: on startup]\n",
    "- disk_{id}_used: Current disk usage percentage (float) [update: every 5s]\n",
    "- disk_{id}_read_rate: Current disk read speed in MB/s (float) [update: every 5s]\n",
    "- disk_{id}_write_rate: Current disk write speed in MB/s (float) [update: every 5s]\n",
    "\n",
    "### 5. Network Metrics\n",
    "- network_interface: Primary network interface name (str) [update: on startup]\n",
    "- ip_address: IP address (str) [update: every 60s]\n",
    "- network_in_rate: Current network input rate in MB/s (float) [update: every 5s]\n",
    "- network_out_rate: Current network output rate in MB/s (float) [update: every 5s]\n",
    "- max_network_in: Maximum input bandwidth in MB/s (float) [update: on startup]\n",
    "- max_network_out: Maximum output bandwidth in MB/s (float) [update: on startup]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Resources Data Model\n",
    "\n",
    "### 1. System Information\n",
    "- hosting_platform: Platform identification (str) [update: on startup]\n",
    "- os_version: Operating system details (str) [update: on startup]\n",
    "- wordslab_version: Software version (str) [update: on startup]\n",
    "- apps_versions: Dictionary of installed apps and versions (json) [update: on startup]\n",
    "- docs_url: Documentation link (str) [update: on startup]\n",
    "\n",
    "### 2. CPU Metrics\n",
    "- cpu_model: Processor name/model (str) [update: on startup]\n",
    "- cpu_cores: Number of cores (int) [update: on startup]\n",
    "- cpu_usage: Current CPU utilization percentage (float) [update: every 5s]\n",
    "- ram_total: Total RAM in GB (float) [update: on startup]\n",
    "- ram_used: Current RAM usage percentage (float) [update: every 5s]\n",
    "\n",
    "### 3. GPU Metrics\n",
    "- gpu_model: GPU name/model (str) [update: on startup]\n",
    "- gpu_flops: Theoretical FLOPS capacity (float) [update: on startup]\n",
    "- gpu_usage: Current GPU utilization percentage (float) [update: every 5s]\n",
    "- vram_total: Total VRAM in GB (float) [update: on startup]\n",
    "- vram_used: Current VRAM usage percentage (float) [update: every 5s]\n",
    "\n",
    "### 4. Storage Metrics\n",
    "[Repeated for disk_id in (1,2,3,4)]\n",
    "- disk_{id}_name: Disk name/mount point (str) [update: on startup]\n",
    "- disk_{id}_total: Total disk space in GB (float) [update: on startup]\n",
    "- disk_{id}_used: Current disk usage percentage (float) [update: every 5s]\n",
    "- disk_{id}_read_rate: Current disk read speed in MB/s (float) [update: every 5s]\n",
    "- disk_{id}_write_rate: Current disk write speed in MB/s (float) [update: every 5s]\n",
    "\n",
    "### 5. Network Metrics\n",
    "- network_interface: Primary network interface name (str) [update: on startup]\n",
    "- ip_address: IP address or DNS name (str) [update: every 60s]\n",
    "- network_in_rate: Current network input rate in MB/s (float) [update: every 5s]\n",
    "- network_out_rate: Current network output rate in MB/s (float) [update: every 5s]\n",
    "- max_network_in: Maximum input bandwidth in MB/s (float) [update: on startup]\n",
    "- max_network_out: Maximum output bandwidth in MB/s (float) [update: on startup]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Development Environment Data Model\n",
    "\n",
    "### 1. Application Links\n",
    "- app_name: Application name (primary key) (str) [update: on startup]\n",
    "- app_url: Access URL (str) [update: every 5s]\n",
    "- app_status: Running status (bool) [update: every 5s]\n",
    "- app_port_var: Environment variable name for port (str) [update: on startup]\n",
    "- app_port_internal: Port number inside VM (int) [update: on startup]\n",
    "- app_port_external: Port number exposed to host (int) [update: on startup]\n",
    "\n",
    "### 2. Project List\n",
    "[For each project in workspace directory]\n",
    "- project_name: Repository name (str) [update: on clone/delete]\n",
    "- project_path: Local directory path (str) [update: on clone/delete]\n",
    "- git_url: Remote repository URL (str) [update: on clone]\n",
    "- disk_size: Project size in MB (float) [update: every 5s]\n",
    "- git_status: Modified/staged files count (int) [update: every 5s]\n",
    "- git_pending_pull: Number of commits to pull (int) [update: every 5s]\n",
    "- git_pending_push: Number of commits to push (int) [update: every 5s]\n",
    "- last_commit: Latest commit timestamp (datetime) [update: every 5s]\n",
    "- last_synced: Latest successful pull/push timestamp (datetime) [update: on sync]\n",
    "- last_updated: Latest file modification timestamp (datetime) [update: every 5s]\n",
    "\n",
    "### 3. AI Models\n",
    "[For each downloaded model]\n",
    "- model_type: Model type (enum: LLM,VLM,Embed,ImgGen,STT,TTS) [update: on download]\n",
    "- model_name: Model identifier (str) [update: on download/delete]\n",
    "- provider: Source platform (enum: Ollama/Huggingface/fastai/Pytorch/Keras) [update: on download]\n",
    "- model_path: Local storage path (str) [update: on download]\n",
    "- disk_size: Model size in MB (float) [update: every 5s]\n",
    "- download_date: Installation timestamp (datetime) [update: on download]\n",
    "- last_used: Last usage timestamp (datetime) [update: on use]\n",
    "- param_count: Number of parameters (int) [update: on download]\n",
    "- quant_dtype: Quantization data type (str) [update: on download]\n",
    "- context_length: Maximum context length (int) [update: on download]\n",
    "- vram_usage: VRAM required in MB (float) [update: on download]\n",
    "- processing_speed: Processing speed (float) [update: every use]\n",
    "- processing_speed_unit: Processing speed unit (str) [update: on download]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AI Services Data Model\n",
    "\n",
    "### 1. Local AI Model Endpoints\n",
    "[For each loaded model]\n",
    "- endpoint_name: Unique service identifier (str) [update: on create]\n",
    "- model_name: Reference to downloaded model - optional (str) [update: on create]\n",
    "- service_type: Type of service (enum: LLM,VLM,Embed,ImgGen,STT,TTS) [update: on create]\n",
    "- inference_server: Server type (enum: vllm,ollama,fastapi) [update: on create]\n",
    "- api_type: API interface (enum: openai,ollama,openAPI) [update: on create]\n",
    "- status: Service state (enum: starting,running,error,stopped) [update: every 5s]\n",
    "- port_internal: Service port inside VM (int) [update: on create]\n",
    "- api_base_url: Service endpoint URL (str) [update: on create]\n",
    "- vram_usage: Current VRAM usage in MB (float) [update: every 5s]\n",
    "- last_error: Last error message if any (str) [update: on error]\n",
    "\n",
    "### 2. External API Endpoints\n",
    "[For each configured API]\n",
    "- provider: Service provider name (str) [update: on configure]\n",
    "- api_name: Service identifier (str) [update: on configure]\n",
    "- api_type: API interface (enum: openai,ollama,openAPI) [update: on configure]\n",
    "- api_base_url: Service endpoint URL (str) [update: on configure]\n",
    "- api_key_var: Environment variable for API key (str) [update: on configure]\n",
    "- api_key: Authentication key (str) [update: on configure]\n",
    "- status: Connection status (bool) [update: every 5s]\n",
    "- quota_limit: API usage limits description (str) [update: on configure]\n",
    "- cost_per_call: Cost structure description (str) [update: on configure]\n",
    "- total_cost: Total cost this month in $ (float) [update: every hour]\n",
    "\n",
    "### 3. External Virtual Machines\n",
    "[For each configured VM]\n",
    "- vm_name: Instance identifier (str) [update: on configure]\n",
    "- instance_type: VM configuration (struct) [update: on configure]\n",
    "  - provider: Cloud provider (enum: runpod.io,vast.ai,jarvislabs.ai) \n",
    "  - region: Datacenter location (str)\n",
    "  - gpu: GPU model (str)\n",
    "  - vram: GPU memory in GB (int)\n",
    "  - vcpu: Number of CPU cores (int)\n",
    "  - ram: CPU memory in GB (int)\n",
    "  - compute_cost: Compute cost in dollar/hour (float)\n",
    "  - container_storage_cost: Container storage cost in dollar/month (float)\n",
    "  - volume_storage_cost: Volume storage cost in dollar/month (float)\n",
    "- container_storage_gb: Container disk size in GB (int) [update: on configure]\n",
    "- volume_storage_gb: Persistent volume size in GB (int) [update: on configure]\n",
    "- status: Instance state (enum: starting,running,stopped,error) [update: every 5s]\n",
    "- terminal_url: VM web terminal URL (str) [update: on configure]\n",
    "- dashboard_url: VM dashboard URL (str) [update: on configure]\n",
    "- last_started: Last start timestamp (datetime) [update: on start]\n",
    "- last_stopped: Last stop timestamp (datetime) [update: on stop]\n",
    "- compute_cost: Total compute cost this month in dollar (float) [update: every hour]\n",
    "- storage_cost: Total storage cost this month in dollar (float) [update: every hour]\n",
    "- total_cost: Total combined cost this month in dollar (float) [update: every hour]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataclasses for Machine Resources\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-28T23:29:47.607569Z",
     "iopub.status.busy": "2025-01-28T23:29:47.606642Z",
     "iopub.status.idle": "2025-01-28T23:29:47.615393Z",
     "shell.execute_reply": "2025-01-28T23:29:47.614785Z",
     "shell.execute_reply.started": "2025-01-28T23:29:47.607538Z"
    }
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Optional\n",
    "from enum import Enum\n",
    "\n",
    "@dataclass\n",
    "class SystemInfo:\n",
    "    hosting_platform: str\n",
    "    os_version: str \n",
    "    wordslab_version: str\n",
    "    apps_versions: Dict[str, str]\n",
    "    docs_url: str\n",
    "\n",
    "@dataclass \n",
    "class CPUMetrics:\n",
    "    cpu_model: str\n",
    "    cpu_cores: int\n",
    "    cpu_usage: float\n",
    "    ram_total: float  # GB\n",
    "    ram_used: float   # GB\n",
    "\n",
    "@dataclass\n",
    "class GPUMetrics:\n",
    "    gpu_model: str\n",
    "    gpu_flops: float\n",
    "    gpu_usage: float  # percentage\n",
    "    vram_total: float # GB\n",
    "    vram_used: float  # GB\n",
    "\n",
    "@dataclass\n",
    "class DiskMetrics:\n",
    "    disk_name: str\n",
    "    disk_mount_points: List[str]\n",
    "    disk_size: float  # GB\n",
    "    disk_used: float   # GB\n",
    "    virtual_disk_file: str\n",
    "    virtual_disk_size: float   # GB\n",
    "    disk_read_rate: float  # MB/s\n",
    "    disk_write_rate: float # MB/s\n",
    "\n",
    "@dataclass\n",
    "class NetworkMetrics:\n",
    "    network_interface: str\n",
    "    ip_address: str\n",
    "    network_in_rate: float  # MB/s\n",
    "    network_out_rate: float # MB/s\n",
    "    max_network_in: float   # MB/s\n",
    "    max_network_out: float  # MB/s\n",
    "\n",
    "@dataclass\n",
    "class MachineResources:\n",
    "    system_info: SystemInfo\n",
    "    cpu_metrics: CPUMetrics\n",
    "    gpu_metrics: GPUMetrics\n",
    "    disk_metrics: List[DiskMetrics]  # Up to 4 disks\n",
    "    network_metrics: NetworkMetrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataclasses for Development Environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-28T23:29:49.692459Z",
     "iopub.status.busy": "2025-01-28T23:29:49.691912Z",
     "iopub.status.idle": "2025-01-28T23:29:49.698070Z",
     "shell.execute_reply": "2025-01-28T23:29:49.697545Z",
     "shell.execute_reply.started": "2025-01-28T23:29:49.692441Z"
    }
   },
   "outputs": [],
   "source": [
    "class ModelType(Enum):\n",
    "    LLM = \"LLM\"\n",
    "    VLM = \"VLM\"\n",
    "    EMBED = \"Embed\"\n",
    "    IMGGEN = \"ImgGen\"\n",
    "    STT = \"STT\"\n",
    "    TTS = \"TTS\"\n",
    "\n",
    "class ModelProvider(Enum):\n",
    "    OLLAMA = \"Ollama\"\n",
    "    HUGGINGFACE = \"Huggingface\"\n",
    "    FASTAI = \"fastai\"\n",
    "    PYTORCH = \"Pytorch\"\n",
    "    KERAS = \"Keras\"\n",
    "\n",
    "@dataclass\n",
    "class AppLink:\n",
    "    app_name: str  # primary key\n",
    "    app_url: str\n",
    "    app_status: bool\n",
    "    app_port_var: str\n",
    "    app_port_internal: int\n",
    "    app_port_external: int\n",
    "\n",
    "@dataclass\n",
    "class Project:\n",
    "    project_name: str\n",
    "    project_path: str\n",
    "    git_url: str\n",
    "    disk_size: float  # MB\n",
    "    git_status: int  # Modified/staged files count\n",
    "    git_pending_pull: bool\n",
    "    git_pending_push: bool\n",
    "    last_commit: datetime\n",
    "    last_synced: datetime\n",
    "    last_updated: datetime\n",
    "\n",
    "@dataclass\n",
    "class AIModel:\n",
    "    model_type: ModelType\n",
    "    model_name: str\n",
    "    provider: ModelProvider\n",
    "    model_path: str\n",
    "    disk_size: float  # MB\n",
    "    download_date: datetime\n",
    "    last_used: datetime\n",
    "    param_count: int\n",
    "    quant_dtype: str\n",
    "    context_length: int\n",
    "    vram_usage: float  # MB\n",
    "    processing_speed: float\n",
    "    processing_speed_unit: str\n",
    "\n",
    "@dataclass\n",
    "class DevelopmentEnvironment:\n",
    "    apps: List[AppLink]\n",
    "    projects: List[Project]\n",
    "    models: List[AIModel]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataclasses for AI services\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-28T23:29:51.405443Z",
     "iopub.status.busy": "2025-01-28T23:29:51.405258Z",
     "iopub.status.idle": "2025-01-28T23:29:51.412152Z",
     "shell.execute_reply": "2025-01-28T23:29:51.411559Z",
     "shell.execute_reply.started": "2025-01-28T23:29:51.405430Z"
    }
   },
   "outputs": [],
   "source": [
    "class InferenceServer(Enum):\n",
    "    VLLM = \"vllm\"\n",
    "    OLLAMA = \"ollama\"\n",
    "    FASTAPI = \"fastapi\"\n",
    "\n",
    "class APIType(Enum):\n",
    "    OPENAI = \"openai\"\n",
    "    OLLAMA = \"ollama\"\n",
    "    OPENAPI = \"openAPI\"\n",
    "\n",
    "class ServiceStatus(Enum):\n",
    "    STARTING = \"starting\"\n",
    "    RUNNING = \"running\"\n",
    "    ERROR = \"error\"\n",
    "    STOPPED = \"stopped\"\n",
    "\n",
    "class CloudProvider(Enum):\n",
    "    RUNPOD = \"runpod.io\"\n",
    "    VAST = \"vast.ai\"\n",
    "    JARVIS = \"jarvislabs.ai\"\n",
    "\n",
    "@dataclass\n",
    "class LocalEndpoint:\n",
    "    endpoint_name: str\n",
    "    model_name: Optional[str]\n",
    "    model_type: ModelType\n",
    "    inference_server: InferenceServer\n",
    "    api_type: APIType\n",
    "    status: ServiceStatus\n",
    "    port_internal: int\n",
    "    api_base_url: str\n",
    "    vram_usage: float  # MB\n",
    "    last_error: Optional[str]\n",
    "\n",
    "@dataclass\n",
    "class ExternalAPI:\n",
    "    provider: str\n",
    "    api_name: str\n",
    "    api_type: APIType\n",
    "    api_base_url: str\n",
    "    api_key_var: str\n",
    "    api_key: str\n",
    "    status: bool\n",
    "    quota_limit: str\n",
    "    cost_per_call: str\n",
    "    total_cost: float  # $ this month\n",
    "\n",
    "@dataclass\n",
    "class VMInstanceType:\n",
    "    provider: CloudProvider\n",
    "    region: str\n",
    "    gpu: str\n",
    "    vram: int  # GB\n",
    "    vcpu: int\n",
    "    ram: int  # GB\n",
    "    compute_cost: float  # $/hour\n",
    "    container_storage_cost: float  # $/month\n",
    "    volume_storage_cost: float  # $/month\n",
    "\n",
    "@dataclass\n",
    "class ExternalVM:\n",
    "    vm_name: str\n",
    "    instance_type: VMInstanceType\n",
    "    container_storage_gb: int\n",
    "    volume_storage_gb: int\n",
    "    status: ServiceStatus\n",
    "    terminal_url: str\n",
    "    dashboard_url: str\n",
    "    last_started: datetime\n",
    "    last_stopped: datetime\n",
    "    compute_cost: float  # $ this month\n",
    "    storage_cost: float  # $ this month\n",
    "    total_cost: float   # $ this month\n",
    "\n",
    "@dataclass\n",
    "class AIServices:\n",
    "    local_endpoints: List[LocalEndpoint]\n",
    "    external_apis: List[ExternalAPI]\n",
    "    external_vms: List[ExternalVM]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rules to compute the Machine Resources dataclasses properties\n",
    "\n",
    "### class SystemInfo\n",
    "\n",
    "- hosting_platform: read the value of the WORDSLAB_PLATFORM environment variable, the possible values are \"WindowsSubsystemForLinux\",\"Jarvislabs.ai\",\"Runpod.io\",\"Vast.ai\",\"UnknownLinux\"\n",
    "\n",
    "- os_version: the application is running on ubuntu, read the Ubuntu version, you should get a value like \"Ubuntu 24.04\"\n",
    "\n",
    "- wordslab_version: read the value of the WORDSLAB_VERSION environment variable, getting a valuye like \"2025-01\"\n",
    "\n",
    "- apps_versions:\n",
    "  - get pytorch version, inserting a value like \"pytorch\":\"2.5.1\" in the dictionary\n",
    "  - get CUDA version: check if a Nvidia GPU is available by first looking if the file \"$WORDSLAB_NOTEBOOKS_ENV/.cpu-only\" exists, and if it doesn't, try to get check the existence of a Nvidia GPU, and then get the CUDA version\n",
    "\n",
    "- docs_url: read the value of the WORDSLAB_ROOT_URL environment variable, and append the value of the environment variable DASHBOARD_PORT, then the path \"/docs\"\n",
    "\n",
    "### class CPUMetrics\n",
    "\n",
    "- cpu_model: running on Ubuntu, get the CPU model name in the format \"12th Intel Code i7-12700H\"\n",
    "\n",
    "- cpu_cores: get the logical processors count\n",
    "\n",
    "- cpu_usage: refresh every 5 sec the % of CPU usage (total for all cores)\n",
    "\n",
    "- ram_total: get the total amount of RAM in GB\n",
    "\n",
    "- ram_used: get the amount of ram used in GB\n",
    "\n",
    "### class GPUMetrics\n",
    "\n",
    "All the properties below should stay empty if there is no Nvidia GPU available on the machine.\n",
    "\n",
    "- gpu_model: get the GPU model description like \"RTX 3070 Ti\"\n",
    "\n",
    "- gpu_flops: use a dictionary \"gpu_model_name\":gpu_flops to get the GPU flops, or leave empty if the GPU model name is not known in the dictionary\n",
    "\n",
    "- gpu_usage: refresh every 5 sec the % of GPU usage\n",
    "\n",
    "- vram_total: get the total amount of memory of the GPU in GB\n",
    "\n",
    "- vram_used: get the amount of GPU vram used in GB\n",
    "\n",
    "### class DiskMetrics\n",
    "\n",
    "We are interested by 4 mount points: /, and the 3 linux paths stored in the following three environement variables: WORDSLAB_HOME, WORDSLAB_WORKSPACE, WORDSLAB_MODELS.\n",
    "\n",
    "We first need to find on which disk these 4 paths (disk_mount_points) are respectively stored (disk_name), then measure for each disk the total size in GB (disk_size), the disk space used in GB (disk_used), the disk read speed in MB/s (disk_read_rate) and write speed in MB/s (disk_write_rate).\n",
    "\n",
    "Special case if WORDSLAB_PLATFORM environment variable equals \"WindowsSubsystemForLinux\":\n",
    "- the mount paths / and WORDSLAB_HOME are stored in the windows file WORDSLAB_WINDOWS_HOME\\virtual-machines\\wordslab-notebooks\\ext4.vhdx\n",
    "- the mount path WORDSLAB_WORKSPACE is stored in the windows file WORDSLAB_WINDOWS_WORKSPACE\\ext4.vhdx\n",
    "- the mount path WORDSLAB_MODELS is stored in WORDSLAB_WINDOWS_MODELS\\ext4.vhdx\n",
    "\n",
    "In that special case: we need to extract the windows disk names (disk_name), \"c:\" or \"d:\" for example, from the windows file path names (virtual_disk_file), and use wsl command line tools to convert the windows path  in a linux path, then measure two things: the windows disk size (disk_size), for example /mnt/c disk size, and the vhdx file size (virtual_disk_size).\n",
    "\n",
    "## class NetworkMetrics:\n",
    "   \n",
    "- network_interface: name of the main network interface\n",
    "\n",
    "- ip_address: ip address of the main network interface\n",
    "\n",
    "- network_in_rate: instantaneous incoming network traffic in MB/s\n",
    "\n",
    "- network_out_rate: instantaneous outgoing network traffic in MB/s\n",
    "\n",
    "- max_network_in: maintain over time the maximum observed value for network_in_rate\n",
    "\n",
    "- max_network_out: maintain over time the maximum observed value for network_out_rate"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
